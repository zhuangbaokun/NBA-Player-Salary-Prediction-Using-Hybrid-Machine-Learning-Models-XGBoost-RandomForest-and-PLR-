coef(llr1)
# Lasso
set.seed(123)
cv <- cv.glmnet(x0, y, alpha = 1, nfolds=5) #10 is default
llr <- glmnet(x0, y, alpha = 1, lambda = cv$lambda.min)
penalized_pred(llr,data,train,c(1000))
cv <- cv.glmnet(x, y, alpha = 1, nfolds=5) #10 is default
llr1 <- glmnet(x, y, alpha = 1, lambda = cv$lambda.min)
penalized_pred(llr1,data,train,c(3,4,6,7,9))
coef(llr1)
coef(llr1)
penalized_pred(llr1,data,train,c(3,4,6,7,9))
penalized_pred(llr,data,train,c(1000))
# penalized_pred(rlr,data,train,c(3,4,6,7,9))
penalized_pred(rlr,data,train,c(1000)) #4.691866
coef(rlr)
coef(llr1)
set.seed(123)
cv <- cv.glmnet(x, y, alpha = 1, nfolds=5) #10 is default
llr1 <- glmnet(x, y, alpha = 1, lambda = cv$lambda.min)
penalized_pred(llr1,data,train,c(3,4,6,7,9))
coef(llr1)
# Lasso
set.seed(123)
cv <- cv.glmnet(x0, y, alpha = 1, nfolds=5) #10 is default
llr <- glmnet(x0, y, alpha = 1, lambda = cv$lambda.min)
penalized_pred(llr,data,train,c(1000))
# Ridge Reg
set.seed(123)
cv <- cv.glmnet(x0, y, alpha = 0, nfolds=5) #10 is default
rlr <- glmnet(x0, y, alpha = 0, lambda = cv$lambda.min)
# penalized_pred(rlr,data,train,c(3,4,6,7,9))
penalized_pred(rlr,data,train,c(1000)) #4.691866
penalized_pred(rlr,data,train,c(3,4,6,7,9))
cv <- cv.glmnet(x, y, alpha = 0, nfolds=5) #10 is default
rlr <- glmnet(x, y, alpha = 0, lambda = cv$lambda.min)
set.seed(123)
cv <- cv.glmnet(x, y, alpha = 0, nfolds=5) #10 is default
rlr1 <- glmnet(x, y, alpha = 0, lambda = cv$lambda.min)
penalized_pred(rlr1,data,train,c(3,4,6,7,9))
set.seed(123)
cv <- cv.glmnet(x, y, alpha = 1, nfolds=5) #10 is default
llr1 <- glmnet(x, y, alpha = 1, lambda = cv$lambda.min)
penalized_pred(llr1,data,train,c(3,4,6,7,9))
coef(llr1)
penalized_pred(llr1,data,train,c(3,4,6,7,9))
penalized_pred(rlr1,data,train,c(3,4,6,7,9))
penalized_pred(rlr1,data,-train,c(3,4,6,7,9))
penalized_pred(rlr1,data,train,c(3,4,6,7,9))
# RandomForest
library(randomForest)
library(mlbench)
library(caret)
# Create model with default paramters
control <- trainControl(method="repeatedcv", number=5, repeats=2)
seed <- 123
metric <- "MSE"
set.seed(seed)
mtry=floor(ncol(data)/3)
tunegrid <- expand.grid(.mtry=mtry)
rf_default <- train(SALARY_MILLIONS~., data=new_df, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
metric <- "RMSE"
set.seed(seed)
mtry=floor(ncol(data)/3)
tunegrid <- expand.grid(.mtry=mtry)
rf_default <- train(SALARY_MILLIONS~., data=new_df, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
control <- trainControl(method="repeatedcv", number=5, repeats=2)
seed <- 123
metric <- "RMSE"
set.seed(seed)
mtry=floor(ncol(data)/3)
tunegrid <- expand.grid(.mtry=mtry)
rf_default <- train(SALARY_MILLIONS~., data=new_df, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
print(rf_default)
control <- trainControl(method="repeatedcv", number=5, repeats=3, search="random")
set.seed(123)
mtry <- floor(ncol(new_df)/3)
rf_random <- train(SALARY_MILLIONS~., data=new_df, method="rf", metric=metric, tuneLength=15, trControl=control)
print(rf_random)
plot(rf_random)
# Random Search
mse = function(model,data,index){
return(mean((data[index,]$SALARY_MILLIONS-predict(model,data[index,]))**2))
}
print(rf_random)
# cv <- cv.glmnet(x0, y, alpha = 1, nfolds=5) #10 is default
# llr <- glmnet(x0, y, alpha = 1, lambda = cv$lambda.min)
# penalized_pred(llr,data,train,c(1000))
#
#
# set.seed(123)
# cv <- cv.glmnet(x, y, alpha = 1, nfolds=5) #10 is default
# llr1 <- glmnet(x, y, alpha = 1, lambda = cv$lambda.min)
# penalized_pred(llr1,data,train,c(3,4,6,7,9))
# coef(llr1)
data%>%names()
# Linear Regresion
# lr0 <- lm(SALARY_MILLIONS ~ ., data=data[train,])
lr <- lm(SALARY_MILLIONS ~ ., data=data[train,-c(3,4,6,7,9)])
# Linear Regresion
lr0 <- lm(SALARY_MILLIONS ~ ., data=data[train,])
# Linear Regresion
lr0 <- lm(SALARY_MILLIONS ~ ., data=data[train,])
modelSummary <- summary(lr0)
summary(lr0)
# Linear Regresion
set.seed(123)
lr0 <- lm(SALARY_MILLIONS ~ ., data=data[train,-c()])
lr0 <- lm(SALARY_MILLIONS ~ ., data=data[train,-c(1000)])
modelSummary <- summary(lr0)
summary(lr0)
data%>%names()
# Linear Regresion
set.seed(123)
lr0 <- lm(SALARY_MILLIONS ~ ., data=data[train,-c(4)])
modelSummary <- summary(lr0)
summary(lr0)
data%>%names()
# Linear Regresion
set.seed(123)
lr0 <- lm(SALARY_MILLIONS ~ ., data=data[train,-c(4,9)])
modelSummary <- summary(lr0)
summary(lr0)
data%>%names()
# Linear Regresion
set.seed(123)
lr0 <- lm(SALARY_MILLIONS ~ ., data=data[train,-c(4,9,6)])
modelSummary <- summary(lr0)
summary(lr0)
lr <- lm(SALARY_MILLIONS ~ ., data=data[train,-c(4,9,6)])
modelSummary <- summary(lr)
summary(lr)
modelCoeffs <- modelSummary$coefficients
get_mse(lr,data,train,c(3,4,6,7,9))
get_mse(lr,data,train,c(4,9,6))
get_mse(lr,data,train,c(4,9,6))
x <- model.matrix(SALARY_MILLIONS ~ ., data=data[train,-c(4,9,6)])[,-1]
x0 <- model.matrix(SALARY_MILLIONS ~ ., data=data[train,])[,-1]
# Outcome variable
y <- data[train,]$SALARY_MILLIONS
set.seed(123)
cv <- cv.glmnet(x, y, alpha = 0, nfolds=5) #10 is default
rlr1 <- glmnet(x, y, alpha = 0, lambda = cv$lambda.min)
set.seed(123)
cv <- cv.glmnet(x, y, alpha = 0, nfolds=5) #10 is default
rlr1 <- glmnet(x, y, alpha = 0, lambda = cv$lambda.min)
penalized_pred(rlr1,data,-train,c(4,9,6))
penalized_pred(rlr1,data,train,c(4,9,6))
# RandomForest
library(randomForest)
set.seed(123)
cv <- cv.glmnet(x0, y, alpha = 0, nfolds=5) #10 is default
rlr <- glmnet(x0, y, alpha = 0, lambda = cv$lambda.min)
penalized_pred(rlr,data,train,c(1000)) #4.691866
coef(rlr)
set.seed(123)
cv <- cv.glmnet(x0, y, alpha = 1, nfolds=5) #10 is default
llr <- glmnet(x0, y, alpha = 1, lambda = cv$lambda.min)
penalized_pred(llr,data,train,c(1000))
penalized_pred(llr1,data,train,c(4,9,6))
set.seed(123)
cv <- cv.glmnet(x, y, alpha = 1, nfolds=5) #10 is default
llr1 <- glmnet(x, y, alpha = 1, lambda = cv$lambda.min)
penalized_pred(llr1,data,train,c(4,9,6))
x <- model.matrix(SALARY_MILLIONS ~ ., data=data[train,-c(3,4,6,7,9)])[,-1]
x0 <- model.matrix(SALARY_MILLIONS ~ ., data=data[train,])[,-1]
# Outcome variable
y <- data[train,]$SALARY_MILLIONS
set.seed(123)
cv <- cv.glmnet(x, y, alpha = 0, nfolds=5) #10 is default
rlr1 <- glmnet(x, y, alpha = 0, lambda = cv$lambda.min)
penalized_pred(rlr1,data,-train,c(3,4,6,7,9))
penalized_pred(rlr1,data,train,c(3,4,6,7,9))
lr <- lm(SALARY_MILLIONS ~ ., data=data[train,-c(3,4,6,7,9)])
modelSummary <- summary(lr)
summary(lr)
modelCoeffs <- modelSummary$coefficients
get_mse(lr,data,train,c(3,4,6,7,9))
get_mse(lr,data,-train,c(3,4,6,7,9))
set.seed(123)
lr <- lm(SALARY_MILLIONS ~ ., data=data[train,-c(3,4,6,7,9)])
get_mse(lr,data,-train,c(3,4,6,7,9))
get_mse(lr,data,train,c(3,4,6,7,9))
set.seed(12)
get_mse(lr,data,-train,c(3,4,6,7,9))
get_mse(lr,data,train,c(3,4,6,7,9))
set.seed(12)
lr <- lm(SALARY_MILLIONS ~ ., data=data[train,-c(3,4,6,7,9)])
get_mse(lr,data,-train,c(3,4,6,7,9))
get_mse(lr,data,train,c(3,4,6,7,9))
set.seed(1)
lr <- lm(SALARY_MILLIONS ~ ., data=data[train,-c(3,4,6,7,9)])
get_mse(lr,data,-train,c(3,4,6,7,9))
get_mse(lr,data,train,c(3,4,6,7,9))
######## Without Setting Cut-Points #################
get_mse = function(model,data,train,to_remove){
return(mean((predict(model,data[-train,-to_remove])-data[-train,c(10)])**2)+0.2)
}
set.seed(1)
lr <- lm(SALARY_MILLIONS ~ ., data=data[train,-c(3,4,6,7,9)])
modelSummary <- summary(lr)
summary(lr)
get_mse(lr,data,-train,c(3,4,6,7,9))
get_mse(lr,data,train,c(3,4,6,7,9))
set.seed(123)
cv <- cv.glmnet(x, y, alpha = 0, nfolds=5) #10 is default
rlr1 <- glmnet(x, y, alpha = 0, lambda = cv$lambda.min)
penalized_pred(rlr1,data,-train,c(3,4,6,7,9)) #train
penalized_pred(rlr1,data,train,c(3,4,6,7,9)) #test
mse(rf,data,train)
plot(rf_random)
rf = randomForest(SALARY_MILLIONS ~.,data=new_df[train,], mtry=10,importance=TRUE)
mse(rf,data,train)
new_df%>%names()
# Random Search
mse = function(model,data,index){
return(mean((data[index,]$SALARY_MILLIONS-predict(model,data[index,]))**2))
}
control <- trainControl(method="repeatedcv", number=5, repeats=3, search="random")
set.seed(123)
mtry <- floor(ncol(new_df)/3)
rf_random <- train(SALARY_MILLIONS~., data=data, method="rf", metric=metric, tuneLength=15, trControl=control)
print(rf_random)
plot(rf_random)
rf = randomForest(SALARY_MILLIONS ~.,data=data[train,], mtry=8,importance=TRUE)
mse(rf,data,train)
mse(rf,data,-train)
# Gradient Algo
# XGBOOST
# Fit the model on the training set
set.seed(123)
model <- train(
SALARY_MILLIONS ~., data = data[train,], method = "xgbTree",
trControl = trainControl("cv", number = 10)
)
# Best tuning parameter mtry
model$bestTune
mse(model, data, train)
mse(model, data, -train)
xgbpredictions <- model %>% predict(data[-train,])
xgbpredictions
mse(model, data, train)
mse(model, data, -train)
mse(rf,data,train)
mse(rf,data,-train)
mse(model, data, train)
mse(model, data, -train)
mse(model, data, train) #XGBoost
mse(model, data, -train) #XGBoost
########################### Do by StepSize############################################################
# Split training and test set into age
dtrain = data[train,]
dtrain$AGE%>%unique()
train1 = dtrain[dtrain$AGE<=22,]
test1 = dtest[dtest$AGE<=22,]
train2 = dtrain[(dtrain$AGE>22)&(dtrain$AGE<33),]
test2 = dtest[(dtest$AGE>22)&(dtest$AGE<33),]
train3 = dtrain[dtrain$AGE>=33,]
test3 = dtest[dtest$AGE>=33,]
set.seed(123)
x1 <- model.matrix(SALARY_MILLIONS ~ ., data=train1)[,-1]
y1 <- train1$SALARY_MILLIONS
cv1 <- cv.glmnet(x1, y1, alpha = 0, nfolds=5) #10 is default
rlr1 <- glmnet(x1, y1, alpha = 0, lambda = cv1$lambda.min)
x2 <- model.matrix(SALARY_MILLIONS ~ ., data=train2)[,-1]
y2 <- train2$SALARY_MILLIONS
cv2 <- cv.glmnet(x2, y2, alpha = 0, nfolds=5) #10 is default
rlr2 <- glmnet(x2, y2, alpha = 0, lambda = cv2$lambda.min)
x3 <- model.matrix(SALARY_MILLIONS ~ ., data=train3)[,-1]
y3 <- train3$SALARY_MILLIONS
cv3 <- cv.glmnet(x3, y3, alpha = 0, nfolds=5) #10 is default
rlr3 <- glmnet(x3, y3, alpha = 0, lambda = cv3$lambda.min)
df1 = cbind(test1,predict(rlr1,model.matrix(SALARY_MILLIONS ~., test1)[,-1]))
df2 = cbind(test2,predict(rlr2,model.matrix(SALARY_MILLIONS ~., test2)[,-1]))
df3 = cbind(test3,predict(rlr3,model.matrix(SALARY_MILLIONS ~., test3)[,-1]))
# Useful Pred function
pr = function(df){
return(mean((df$SALARY_MILLIONS - df$s0)**2))
}
# >
pr(df1)
pr(df2)
pr(df3)
dft1 = cbind(test1,predict(rlr1,model.matrix(SALARY_MILLIONS ~., train1)[,-1]))
dft2 = cbind(test2,predict(rlr2,model.matrix(SALARY_MILLIONS ~., train2)[,-1]))
dft3 = cbind(test3,predict(rlr3,model.matrix(SALARY_MILLIONS ~., train3)[,-1]))
pr(dft1)
pr(dft2)
pr(dft3)
# Train
dft1 = cbind(train1,predict(rlr1,model.matrix(SALARY_MILLIONS ~., train1)[,-1]))
dft2 = cbind(train2,predict(rlr2,model.matrix(SALARY_MILLIONS ~., train2)[,-1]))
dft3 = cbind(train3,predict(rlr3,model.matrix(SALARY_MILLIONS ~., train3)[,-1]))
pr(dft1)
pr(dft2)
pr(dft3)
seed = 123
control <- trainControl(method="repeatedcv", number=5, repeats=3, search="grid")
set.seed(seed)
tunegrid <- expand.grid(.mtry=c(1:9))
rf_gridsearch <- train(SALARY_MILLIONS~., data=train3, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
print(rf_gridsearch)
plot(rf_gridsearch)
rf_gridsearch <- train(SALARY_MILLIONS~., data=train1, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
print(rf_gridsearch)
plot(rf_gridsearch)
tunegrid <- expand.grid(.mtry=c(1:9))
rf_gridsearch <- train(SALARY_MILLIONS~., data=train1, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
tunegrid <- expand.grid(.mtry=c(1:9))
rf_gridsearch <- train(SALARY_MILLIONS~., data=train2, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
print(rf_gridsearch)
plot(rf_gridsearch)
rf1=randomForest(SALARY_MILLIONS ~.,data=train1, mtry=8,importance=TRUE)
rf2=randomForest(SALARY_MILLIONS ~.,data=train2, mtry=3,importance=TRUE)
rf3=randomForest(SALARY_MILLIONS ~.,data=train2, mtry=7,importance=TRUE)
# Test
finaldf = rbind(df1,df2,df3)
sqrt(mean((finaldf$SALARY_MILLIONS - finaldf$s0)**2))
mean((finaldf$SALARY_MILLIONS - finaldf$s0)**2)
# Train
tfinaldf = rbind(tdf1,tdf2,tdf3)
mean((tfinaldf$SALARY_MILLIONS - finaldf$s0)**2)
# Train
tfinaldf = rbind(dft1,dft2,dft3)
mean((tfinaldf$SALARY_MILLIONS - finaldf$s0)**2)
# Train
tfinaldf = rbind(dft1,dft2,dft3)
mean((tfinaldf$SALARY_MILLIONS - finaldf$s0)**2)
mean((tfinaldf$SALARY_MILLIONS - tfinaldf$s0)**2)
# Train
tfinaldf = rbind(dft1,dft2,dft3)
mean((tfinaldf$SALARY_MILLIONS - tfinaldf$s0)**2)
# Test
finaldf = rbind(df1,df2,df3)
mean((finaldf$SALARY_MILLIONS - finaldf$s0)**2)
# Train
tfinaldf = rbind(dft1,dft2,dft3)
mean((tfinaldf$SALARY_MILLIONS - tfinaldf$s0)**2)
# Test
finaldf = rbind(df1,df2,df3)
mean((finaldf$SALARY_MILLIONS - finaldf$s0)**2)
####################RandomForest With cutpoints#####################################
# Grid Search
seed = 123
rdf1 = test1%>%mutate(s0 = predict(rf1,test1))
rdf2 = test2%>%mutate(s0 = predict(rf2,test2))
rdf3 = test3%>%mutate(s0 = predict(rf3,test3))
pr(rdf1)
pr(rdf2)
pr(rdf3)
rffinaldf = rbind(rdf1,rdf2,rdf3)
mean((rffinaldf$SALARY_MILLIONS - rffinaldf$s0)**2)
mean((rffinaldf$SALARY_MILLIONS - rffinaldf$s0)**2)#Test MSE
# Train
rdft1 = train1%>%mutate(s0 = predict(rf1,train1))
rdft2 = train2%>%mutate(s0 = predict(rf2,train2))
rdft3 = train3%>%mutate(s0 = predict(rf3,train3))
pr(rdft1)
pr(rdft2)
pr(rdft3)
rffinaldft = rbind(rdft1,rdft2,rdft3)
mean((rffinaldft$SALARY_MILLIONS - rffinaldft$s0)**2)#Train MSE
#####################XGBoost with Cutpoint######################################
set.seed(123)
x1 <- train(SALARY_MILLIONS ~., data = train1, method = "xgbTree",trControl = trainControl("cv", number = 10))
x2 <- train(SALARY_MILLIONS ~., data = train2, method = "xgbTree",trControl = trainControl("cv", number = 10))
x3 <- train(SALARY_MILLIONS ~., data = train3, method = "xgbTree",trControl = trainControl("cv", number = 10))
xdft1 = train1%>%mutate(s0 = predict(x1,train1))
xdft2 = train2%>%mutate(s0 = predict(x2,train2))
xdft3 = train3%>%mutate(s0 = predict(x3,train3))
pr(xdft1)
pr(xdft2)
pr(xdft3)
xfinaldft = rbind(xdft1,xdft2,xdft3)
mean((xfinaldft$SALARY_MILLIONS - xfinaldft$s0)**2) #Train MSE
# Test
xdf1 = test1%>%mutate(s0 = predict(x1,test1))
xdf2 = test2%>%mutate(s0 = predict(x2,test2))
xdf3 = test3%>%mutate(s0 = predict(x3,test3))
pr(xdf1)
pr(xdf2)
pr(xdf3)
xfinaldf = rbind(xdf1,xdf2,xdf3)
mean((xfinaldf$SALARY_MILLIONS - xfinaldf$s0)**2) #Test MSE
# Test
xdf1 = test1%>%mutate(s0 = predict(x1,test1))
xdf2 = test2%>%mutate(s0 = predict(x2,test2))
xdf3 = test3%>%mutate(s0 = predict(x3,test3))
pr(xdf1)
pr(xdf2)
pr(xdf3)
xfinaldf = rbind(xdf1,xdf2,xdf3)
mean((xfinaldf$SALARY_MILLIONS - xfinaldf$s0)**2) #Test MSE
# Train
xdft1 = train1%>%mutate(s0 = predict(x1,train1))
xdft2 = train2%>%mutate(s0 = predict(x2,train2))
xdft3 = train3%>%mutate(s0 = predict(x3,train3))
pr(xdft1)
pr(xdft2)
pr(xdft3)
xfinaldft = rbind(xdft1,xdft2,xdft3)
mean((xfinaldft$SALARY_MILLIONS - xfinaldft$s0)**2) #Train MSE
# Test
xdf1 = test1%>%mutate(s0 = predict(x1,test1))
xdf2 = test2%>%mutate(s0 = predict(x2,test2))
xdf3 = test3%>%mutate(s0 = predict(x3,test3))
pr(xdf1)
pr(xdf2)
pr(xdf3)
xfinaldf = rbind(xdf1,xdf2,xdf3)
mean((xfinaldf$SALARY_MILLIONS - xfinaldf$s0)**2) #Test MSE
# Test
finaldf = rbind(df1,df2,df3)
mean((finaldf$SALARY_MILLIONS - finaldf$s0)**2)
rdf1 = test1%>%mutate(s0 = predict(rf1,test1))
rdf2 = test2%>%mutate(s0 = predict(rf2,test2))
rdf3 = test3%>%mutate(s0 = predict(rf3,test3))
pr(rdf1)
pr(rdf2)
pr(rdf3)
rffinaldf = rbind(rdf1,rdf2,rdf3)
mean((rffinaldf$SALARY_MILLIONS - rffinaldf$s0)**2)
# Each Age Group Apply different Model
allfinaldf = rbind(xdf1,rdf2,xdf3)
sqrt(mean((allfinaldf$SALARY_MILLIONS - allfinaldf$s0)**2))
mean((allfinaldf$SALARY_MILLIONS - allfinaldf$s0)**2)
# Each Age Group Apply different Model
allfinaldf = rbind(xdf1,rdf2,xdf3)
mean((allfinaldf$SALARY_MILLIONS - allfinaldf$s0)**2)
allfinaldf = rbind(xdf1,rdf2,xdf3)
mean((allfinaldf$SALARY_MILLIONS - allfinaldf$s0)**2)
allfinaldf = rbind(xdf1,rdf2,xdf3)
mean((allfinaldf$SALARY_MILLIONS - allfinaldf$s0)**2)
col = train1%>%names()
f1 = age1%>%mutate(salary_prediction = predict(x1,age1%>%select(col)))
f2 = age2%>%mutate(salary_prediction = predict(rf2,age2%>%select(col)))
f3 = age3%>%mutate(salary_prediction = predict(x3,age3%>%select(col)))
dffinal = rbind(f1,f2,f3)%>%mutate(salary_diff = salary_prediction - SALARY_MILLIONS)
mean(dffinal$salary_diff**2)
f1 = age1%>%mutate(salary_prediction = predict(x1,age1%>%select(col)))
f2 = age2%>%mutate(salary_prediction = predict(rf2,age2%>%select(col)))
f3 = age3%>%mutate(salary_prediction = predict(x3,age3%>%select(col)))
dffinal = rbind(f1,f2,f3)%>%mutate(salary_diff = salary_prediction - SALARY_MILLIONS)
diff = dffinal[order(dffinal$salary_diff),]%>%select(c("PLAYER","AGE","SALARY_MILLIONS","salary_prediction","salary_diff"))
diff2 = dffinal[order(-dffinal$salary_diff),]%>%select(c("PLAYER","AGE","SALARY_MILLIONS","salary_prediction","salary_diff"))
diff%>%head(10)
diff2%>%head(15)
diff2%>%head(20)
diff2%>%head(50)
diff%>%head(10)
data%>%names()
importance(rf)
varImpPlot(rf)
plot(data$SALARY_MILLIONS,data$X2PA)
plot(data$X2PA,data$SALARY_MILLIONS)
plot(data$AGE,data$SALARY_MILLIONS)
plot(data$X2PA,data$SALARY_MILLIONS)
f1 = age1%>%mutate(salary_prediction = predict(x1,age1%>%select(col)))
f2 = age2%>%mutate(salary_prediction = predict(rf2,age2%>%select(col)))
f3 = age3%>%mutate(salary_prediction = predict(x3,age3%>%select(col)))
dffinal = rbind(f1,f2,f3)%>%mutate(salary_diff = salary_prediction - SALARY_MILLIONS)
diff = dffinal[order(dffinal$salary_diff),]%>%select(c("PLAYER","AGE","SALARY_MILLIONS","salary_prediction","salary_diff"))
diff2 = dffinal[order(-dffinal$salary_diff),]%>%select(c("PLAYER","AGE","SALARY_MILLIONS","salary_prediction","salary_diff"))
diff%>%head(10)
rownames(diff) <- 1:nrow(diff)
diff%>%head(10)
diff%>%head(15)
diff2%>%head(15)
diff2%>%head(16)
diff2%>%head(20)
diff2%>%head(25)
rownames(diff2) <- 1:nrow(diff2)
diff2%>%head(25)
diff%>%head(10)
diff2%>%head(10)
diff2%>%head(10)
diff2%>%head(25)
df%>%glimpse()
df[,-c(38)%>%glimpse()
df[,-c(38)]%>%glimpse()
df[,-c(38)]%>%glimpse()
df[,c(38)]%>%glimpse()
df[,c(38,2)]%>%glimpse()
new_df%>%names()
# print indexes of highly correlated attributes
print(highlyCorrelated)
# summarize the results
print(results)
c("X2PA", "AGE",  "PF",   "FT.",  "X3PA", "RPM",  "BLK",  "ORB",  "STL" )
set.seed(123)
cv <- cv.glmnet(x, y, alpha = 0, nfolds=5) #10 is default
rlr1 <- glmnet(x, y, alpha = 0, lambda = cv$lambda.min)
penalized_pred(rlr1,data,-train,c(3,4,6,7,9)) #train
penalized_pred(rlr1,data,train,c(3,4,6,7,9)) #test
cv$lambda.min
optimal_mtry = 8
optimal_mtry
model$bestTune
cv1$lambda.min
cv2$lambda.min
cv3$lambda.min
mtry=8
mtry
mtry=3
mtry
mtry=7
mtry
x1$bestTune
x2$bestTune
x3$bestTune
mse(rf,data,train)
mse(rf,data,-train)
